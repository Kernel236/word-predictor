<div align="center">

# ğŸ”® Word Predictor: N-gram Language Model

[![R](https://img.shields.io/badge/R-276DC3?style=for-the-badge&logo=r&logoColor=white)](https://www.r-project.org/)
[![RStudio](https://img.shields.io/badge/RStudio-4285F4?style=for-the-badge&logo=rstudio&logoColor=white)](https://rstudio.com/)
[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/Kernel236/word-predictor)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=flat-square)](https://opensource.org/licenses/MIT)
[![R Version](https://img.shields.io/badge/R-%3E%3D4.0-blue.svg?style=flat-square)](https://www.r-project.org/)
[![Project Status](https://img.shields.io/badge/Status-Active%20Development-green.svg?style=flat-square)](https://github.com/Kernel236/word-predictor)
[![Last Updated](https://img.shields.io/badge/Last%20Updated-October%202025-orange.svg?style=flat-square)](https://github.com/Kernel236/word-predictor)


</div>

---

## ğŸ¯ Project Overview

This project implements a **next-word prediction model** based on statistical n-gram analysis. The system analyzes frequency patterns in unigrams, bigrams, and trigrams to build a predictive language model suitable for text completion applications.

## ğŸ—‚ï¸ Repository Structure

```
word-predictor/
â”œâ”€â”€ README.md                          # Project documentation
â”œâ”€â”€ word-predictor.Rproj              # RStudio project configuration
â”œâ”€â”€ .gitignore                        # Git ignore rules
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # Original text corpora (git-ignored)
â”‚   â”‚   â”œâ”€â”€ en_US.blogs.txt           # English blogs (~200MB)
â”‚   â”‚   â”œâ”€â”€ en_US.news.txt            # English news (~200MB)
â”‚   â”‚   â”œâ”€â”€ en_US.twitter.txt         # English tweets (~160MB)
â”‚   â”‚   â”œâ”€â”€ de_DE.* / fi_FI.* / ru_RU.*  # Other languages
â”‚   â””â”€â”€ processed/                    # Generated frequency tables (git-ignored)
â”‚       â”œâ”€â”€ freq_uni_en_top100k.rds   # Unigram frequencies
â”‚       â”œâ”€â”€ freq_bi_top100k.rds       # Bigram frequencies
â”‚       â”œâ”€â”€ freq_tri_top100k.rds      # Trigram frequencies
â”‚       â”œâ”€â”€ uni_lookup.rds            # Pruned unigram lookup
â”‚       â”œâ”€â”€ bi_pruned.rds             # Pruned bigram lookup
â”‚       â”œâ”€â”€ tri_pruned.rds            # Pruned trigram lookup
â”‚       â””â”€â”€ lang_meta.rds             # Model metadata
â”œâ”€â”€ R/
â”‚   â”œâ”€â”€ eda.R                         # Exploratory data analysis
â”‚   â”œâ”€â”€ build_lang_model.R            # Language model construction
â”‚   â””â”€â”€ predict_demo.R                # Prediction algorithm demo
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ eda.Rmd                       # EDA report (R Markdown)
â”‚   â””â”€â”€ cache/                        # Cached results (git-ignored)
â”œâ”€â”€ docs/                             # GitHub Pages (for HTML report)
â”‚   â””â”€â”€ index.html                    # Published EDA report
â””â”€â”€ app/                              # Shiny application (coming soon)
```

## ğŸ“– Methodology

### 1ï¸âƒ£ Data Preprocessing
- ğŸ“š **Corpus Loading**: Multi-source English text data (blogs, news, social media)
- ğŸ§¹ **Text Cleaning**: Standardization, tokenization, and quality filtering
- ğŸ¯ **Sampling Strategy**: Stratified 5% sample maintaining source proportions

### 2ï¸âƒ£ N-gram Analysis
- âœ‚ï¸ **Tokenization**: Generation of unigrams, bigrams, and trigrams
- ğŸ“Š **Frequency Calculation**: Statistical analysis of term occurrence patterns
- ğŸ“ˆ **Coverage Analysis**: Vocabulary size requirements for different coverage thresholds

### 3ï¸âƒ£ Statistical Validation
- ğŸ“ **Zipf's Law Verification**: Confirming power-law distributions in natural language
- ğŸ•³ï¸ **Sparsity Assessment**: Understanding vocabulary growth with n-gram order
- âš¡ **Coverage Efficiency**: Determining optimal vocabulary sizes

## ğŸ“Š Key Findings

<div align="center">

| N-gram Type | 50% Coverage | 90% Coverage | Key Insight |
|-------------|--------------|--------------|-------------|
| ğŸ”¤ **Unigrams** | ~100 terms | ~1,000 terms | High efficiency |
| ğŸ”¤ğŸ”¤ **Bigrams** | ~1,000 terms | ~10,000 terms | Moderate growth |
| ğŸ”¤ğŸ”¤ğŸ”¤ **Trigrams** | ~10,000+ terms | ~100,000+ terms | Exponential explosion |

</div>

## ğŸ”„ Project Workflow

### Phase 1: Data Exploration âœ…
- **Script**: `R/eda.R`
- **Output**: `reports/eda.Rmd` â†’ `docs/index.html`
- **Purpose**: Understand data patterns, validate approach, identify optimization opportunities

### Phase 2: Model Building âœ…
- **Script**: `R/build_lang_model.R`
- **Output**: Pruned n-gram lookup tables in `data/processed/`
- **Purpose**: Create optimized language model with frequency pruning and backoff structure

### Phase 3: Prediction Algorithm âœ…
- **Script**: `R/predict_demo.R`
- **Output**: Working prediction function with demo
- **Purpose**: Implement and test stupid backoff algorithm for next-word prediction

### Phase 4: Shiny Application ğŸš§
- **Location**: `app/` (in development)
- **Features**: Interactive UI, live predictions, performance metrics
- **Purpose**: User-friendly web application for text prediction

## ğŸš€ Quick Start

### Prerequisites
```bash
# R version 4.0+ required
R --version
```

### Installation
```bash
# Clone the repository
git clone https://github.com/Kernel236/word-predictor.git
cd word-predictor

# Open RStudio project
open word-predictor.Rproj
```

### Run Analysis
```r
# 1. Exploratory Data Analysis
source("R/eda.R")

# 2. Generate HTML report
rmarkdown::render("reports/eda.Rmd", output_dir = "docs", output_file = "index.html")

# 3. Build language model (full corpus)
source("R/build_lang_model.R")

# 4. Test prediction algorithm
source("R/predict_demo.R")
```

## âš™ï¸ Technical Dependencies

### ğŸ“¦ R Packages
```r
# Core dependencies
library(Rcaptext)  # ğŸ› ï¸ Custom text processing and n-gram analysis
library(dplyr)     # ğŸ“Š Data manipulation and transformation  
library(tidyr)     # ğŸ”„ Data reshaping and organization
library(ggplot2)   # ğŸ“ˆ Statistical visualization
library(knitr)     # ğŸ“– Report generation
```

### ğŸ’» System Requirements
- ğŸ–¥ï¸ **RAM**: Minimum 1GB available for processing
- ğŸ“Š **R Version**: 4.0+ recommended
- ğŸ“š **Data**: English US text corpora (included in raw data)
- ğŸ’¾ **Storage**: ~500MB for processed frequency tables

## ï¿½ Rcaptext Package: Custom Text Processing Engine

This project is powered by **Rcaptext**, a specialized R package developed specifically for text processing and n-gram analysis. The package provides a suite of functions tailored for natural language processing tasks of this project.

### ğŸŒŸ Key Features (so far...)

- ğŸ“š **`load_corpus()`** - Efficient multi-file corpus loading with automatic source tagging
- ğŸ§¹ **`clean_text()`** - Advanced text preprocessing with configurable cleaning rules
- ğŸ² **`sample_corpus()`** - Stratified sampling maintaining source proportions
- âœ‚ï¸ **`tokenize_unigrams()`**, **`tokenize_bigrams()`**, **`tokenize_trigrams()`** - Optimized n-gram tokenization
- ğŸ“Š **`freq_unigrams()`**, **`freq_bigrams()`**, **`freq_trigrams()`** - Statistical frequency analysis
- ğŸ“ˆ **`coverage_from_freq()`** - Coverage analysis at specified thresholds
- ğŸ¨ **`plot_top_terms()`**, **`plot_rank_frequency()`**, **`plot_cumulative_coverage()`** - Rich visualization functions

### ğŸ“¦ Installation & Usage

```r
# Install development version
devtools::install_github("Kernel236/Rcaptext")

# Load the package
library(Rcaptext)

# Example workflow
corpus <- load_corpus("en_US", base_dir = "data/raw")
clean_corpus <- clean_text(corpus$text)
sample_data <- sample_corpus(corpus, prop = 0.05)
```

> ğŸ”¬ **Development Status**: Rcaptext is actively developed alongside this project, with new features and optimizations added based on real-world text processing needs.

## ï¿½ğŸ“‹ Advanced Usage

### ğŸ”¬ Running the Analysis
```r
# Load the project and run full analysis
source("R/eda.R")

# Generate EDA report (uses caching for improved performance)
rmarkdown::render("reports/eda.Rmd")

# Clear cache if needed (for fresh run)
knitr::clean_cache("reports/eda.Rmd")
```

> ğŸ’¡ **Performance Tip**: The R Markdown report uses intelligent caching to optimize knitting performance.
> 
> - â±ï¸ **First run**: Full processing time (may take several minutes)  
> - âš¡ **Subsequent runs**: Much faster execution using cached results
> - ğŸ“ **Cache location**: `reports/cache/` directory (ignored by git)

### ğŸ“Š Using Processed Data
```r
# Load pre-computed frequency tables
freq_uni <- readRDS("data/processed/freq_uni_en_top100k.rds")
freq_bi  <- readRDS("data/processed/freq_bi_top100k.rds")
freq_tri <- readRDS("data/processed/freq_tri_top100k.rds")
```

## ğŸš€ Performance Optimization

### ğŸ’¾ Caching System
- ğŸ”„ **Automatic Caching**: Heavy computations cached using knitr's caching system
- ğŸ¯ **Dependency Tracking**: Smart cache invalidation based on chunk dependencies  
- ğŸ“ **Storage**: Cache files in `reports/cache/` (excluded from version control)
- âš¡ **Benefits**: Reduces report generation from minutes to seconds after first run

### ğŸ§  Memory Management
- ğŸ—‘ï¸ **Garbage Collection**: Automatic cleanup of large temporary objects
- ğŸ² **Strategic Sampling**: Uses 5% corpus sample for optimal analysis depth vs performance
- ğŸ“¦ **Efficient Storage**: RDS format for compressed serialization of frequency tables


## ğŸ¤ Contributing

Contributions are welcome! Please ensure all code follows the established style conventions and includes appropriate documentation.

### ğŸ’¡ How to Contribute
1. ğŸ´ Fork the repository
2. ğŸŒŸ Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. ğŸ’¾ Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. ğŸ“¤ Push to the branch (`git push origin feature/AmazingFeature`)
5. ğŸ”„ Open a Pull Request

## ğŸ“„ License

This project is developed for educational and research purposes. Please respect the licensing terms of the original corpora datasets.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

<div align="center">

### ğŸ‘¨â€ğŸ’» Author

**Kernel236**

[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/Kernel236)

---

*Last Updated: October 2025 | Project Status: ğŸŸ¢ Active Development*

</div>